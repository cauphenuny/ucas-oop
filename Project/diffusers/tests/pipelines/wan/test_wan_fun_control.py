# Copyright 2025 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Tests for WanFunControl pipeline and transformer.

This test suite validates that the diffusers implementation of Wan Fun-Control
produces intermediate features that match the reference fixtures generated by
DiffSynth-Studio's WanVideo pipeline.

Fixture path: tests/fixtures/wan21_fun_v11_control_camera.pt
Fixture metadata: tests/fixtures/wan21_fun_v11_control_camera.json

Expected intermediate features:
1. image_vae_embedding: [1, 20, 21, 60, 104] - VAE encoded input image
2. control_condition_embedding: [1, 36, 21, 60, 104] - Camera control embeddings
3. patchify_tensor: [1, 1536, 21, 30, 52] - Patchified output with control
4. patch_sequence: [1, 32760, 1536] - Reshaped patches (32760 = 21*30*52)
5. blocks: Transformer block outputs at intervals (0, 6, 12, 18, 24)
"""

import json
import unittest
from pathlib import Path

import torch
import torch.nn as nn

from diffusers.models import WanFunControlTransformer3DModel


def assert_tensors_close(actual, expected, name, rtol=1e-3, atol=1e-5):
    """
    Assert two tensors are close within tolerance.
    
    Args:
        actual: Actual tensor from our implementation
        expected: Expected tensor from fixture
        name: Name of the tensor for error messages
        rtol: Relative tolerance
        atol: Absolute tolerance
    """
    assert actual.shape == expected.shape, (
        f"{name} shape mismatch: expected {expected.shape}, got {actual.shape}"
    )
    
    # Convert to same dtype for comparison
    actual_float = actual.float()
    expected_float = expected.float()
    
    # Check if tensors are close
    if not torch.allclose(actual_float, expected_float, rtol=rtol, atol=atol):
        max_diff = (actual_float - expected_float).abs().max().item()
        mean_diff = (actual_float - expected_float).abs().mean().item()
        raise AssertionError(
            f"{name} values don't match:\n"
            f"  Max diff: {max_diff}\n"
            f"  Mean diff: {mean_diff}\n"
            f"  Tolerance: rtol={rtol}, atol={atol}"
        )


class WanFunControlFixtureMatchTests(unittest.TestCase):
    """Test that WanFunControl implementation matches fixture outputs exactly."""

    @classmethod
    def setUpClass(cls):
        """Load fixtures and prepare test data."""
        fixture_dir = Path(__file__).parent.parent.parent.parent.parent / "tests" / "fixtures"
        fixture_path = fixture_dir / "wan21_fun_v11_control_camera.pt"
        metadata_path = fixture_dir / "wan21_fun_v11_control_camera.json"
        
        if not fixture_path.exists():
            raise FileNotFoundError(f"Fixture not found: {fixture_path}")
        
        cls.fixture = torch.load(fixture_path, map_location="cpu")
        
        with open(metadata_path) as f:
            cls.metadata = json.load(f)
        
        # Extract dimensions from fixture
        cls.num_frames = cls.metadata['num_frames']
        cls.height = cls.metadata['height']
        cls.width = cls.metadata['width']
        
        # Model configuration for Wan2.1-Fun-V1.1-1.3B-Control-Camera
        cls.model_config = {
            "patch_size": (1, 2, 2),
            "num_attention_heads": 12,
            "attention_head_dim": 128,
            "in_channels": 20,
            "control_channels": 36,
            "out_channels": 20,
            "text_dim": 4096,
            "freq_dim": 256,
            "ffn_dim": 4608,
            "num_layers": 28,
            "cross_attn_norm": True,
            "qk_norm": "rms_norm_across_heads",
            "eps": 1e-6,
        }
        
    def test_01_patchify_output_matches_fixture(self):
        """
        Test that patchify() produces output matching the fixture.
        
        This test validates:
        1. Input latents (20ch) + control latents (36ch) are correctly concatenated
        2. Conv3d patchification produces the expected output
        3. Output matches fixture patchify_tensor exactly
        """
        # Create model with same architecture as fixture
        model = WanFunControlTransformer3DModel(**self.model_config)
        
        # We need to initialize the patch_embedding weights to match the fixture
        # For now, we'll test with random weights to validate the architecture
        # In a real scenario, weights would be loaded from pre-trained checkpoint
        
        # Get input tensors - we'd need the actual input image and control from DiffSynth
        # For this test, we create dummy inputs with the same shape
        image_latents = self.fixture["image_vae_embedding"]
        control_latents = self.fixture["control_condition_embedding"]
        
        # Run patchify
        with torch.no_grad():
            patchified = model.patchify(image_latents, control_latents)
        
        expected_patchified = self.fixture["patchify_tensor"]
        
        # For now, we can only validate shape since we don't have the exact weights
        # In production, this would validate exact values after loading weights
        self.assertEqual(
            list(patchified.shape),
            list(expected_patchified.shape),
            "Patchify output shape must match fixture"
        )
        
        # TODO: Once weights are available, uncomment this to validate values:
        # assert_tensors_close(patchified, expected_patchified, "patchify_tensor")
        
    def test_02_patch_sequence_matches_fixture(self):
        """
        Test that reshaping patchify output produces correct sequence.
        
        Validates: [B, C, F, H, W] -> [B, F*H*W, C] transformation
        """
        patchify_tensor = self.fixture["patchify_tensor"]
        expected_sequence = self.fixture["patch_sequence"]
        
        # Reshape: [1, 1536, 21, 30, 52] -> [1, 32760, 1536]
        batch, channels, frames, height, width = patchify_tensor.shape
        actual_sequence = patchify_tensor.flatten(2).transpose(1, 2)
        
        # Validate shape
        self.assertEqual(
            list(actual_sequence.shape),
            list(expected_sequence.shape),
            "Patch sequence shape must match fixture"
        )
        
        # Validate values - this should match exactly since it's just a reshape
        assert_tensors_close(
            actual_sequence,
            expected_sequence,
            "patch_sequence",
            rtol=1e-6,
            atol=1e-8
        )
        
    def test_03_transformer_forward_with_dummy_inputs(self):
        """
        Test transformer forward pass with dummy inputs.
        
        This validates the model can process inputs correctly, even without
        matching the exact fixture values (which requires pre-trained weights).
        """
        model = WanFunControlTransformer3DModel(**self.model_config)
        model.eval()
        
        # Create dummy inputs matching fixture shapes
        batch_size = 1
        latent_frames = 21
        latent_height = 60
        latent_width = 104
        
        # Dummy latents
        image_latents = torch.randn(batch_size, 20, latent_frames, latent_height, latent_width)
        control_latents = torch.randn(batch_size, 36, latent_frames, latent_height, latent_width)
        
        # Dummy timestep and text embeddings
        timestep = torch.tensor([0], dtype=torch.long)
        text_embeddings = torch.randn(batch_size, 226, 4096)  # UMT5 embeddings
        
        # Forward pass
        with torch.no_grad():
            output = model(
                hidden_states=image_latents,
                timestep=timestep,
                encoder_hidden_states=text_embeddings,
                control_camera_latents=control_latents,
                return_dict=True
            )
        
        # Validate output shape
        # Output should be [batch, out_channels, frames, height, width]
        expected_shape = [batch_size, 20, latent_frames, latent_height, latent_width]
        self.assertEqual(
            list(output.sample.shape),
            expected_shape,
            "Transformer output shape must match expected dimensions"
        )
        
    def test_04_intermediate_block_outputs_with_hooks(self):
        """
        Test that intermediate transformer block outputs can be captured.
        
        This test validates that we can hook into transformer blocks to capture
        intermediate activations, which is necessary for comparing against
        fixture block outputs (block_000, block_006, etc.).
        """
        model = WanFunControlTransformer3DModel(**self.model_config)
        model.eval()
        
        # Storage for intermediate outputs
        block_outputs = {}
        
        # Register forward hooks to capture block outputs
        def make_hook(block_idx):
            def hook(module, input, output):
                # Store the hidden states output
                block_outputs[f"block_{block_idx:03d}"] = output.detach().cpu()
            return hook
        
        # Register hooks at the same intervals as fixture (0, 6, 12, 18, 24)
        block_interval = self.metadata.get("block_interval", 6)
        hooks = []
        for i in range(0, len(model.blocks), block_interval):
            hook = model.blocks[i].register_forward_hook(make_hook(i))
            hooks.append(hook)
        
        # Create dummy inputs
        batch_size = 1
        latent_frames = 21
        latent_height = 60
        latent_width = 104
        
        image_latents = torch.randn(batch_size, 20, latent_frames, latent_height, latent_width)
        control_latents = torch.randn(batch_size, 36, latent_frames, latent_height, latent_width)
        timestep = torch.tensor([0], dtype=torch.long)
        text_embeddings = torch.randn(batch_size, 226, 4096)
        
        # Forward pass to trigger hooks
        with torch.no_grad():
            _ = model(
                hidden_states=image_latents,
                timestep=timestep,
                encoder_hidden_states=text_embeddings,
                control_camera_latents=control_latents,
                return_dict=True
            )
        
        # Clean up hooks
        for hook in hooks:
            hook.remove()
        
        # Validate that we captured the expected blocks
        expected_blocks = [f"block_{i:03d}" for i in range(0, 25, block_interval)]
        self.assertEqual(
            sorted(block_outputs.keys()),
            sorted(expected_blocks),
            "Should capture same block indices as fixture"
        )
        
        # Validate shapes (values would require pre-trained weights)
        for block_name in expected_blocks:
            if block_name in self.fixture["blocks"]:
                expected_shape = list(self.fixture["blocks"][block_name].shape)
                actual_shape = list(block_outputs[block_name].shape)
                # Note: shapes might differ slightly depending on exact architecture
                # The sequence length (dim 1) should match: 32760
                self.assertEqual(
                    actual_shape[1],
                    expected_shape[1],
                    f"{block_name} sequence length must match fixture"
                )


class WanFunControlIntegrationTests(unittest.TestCase):
    """
    Integration tests for full pipeline matching fixture outputs.
    
    These tests would require:
    1. Pre-trained model weights
    2. Actual input image/video used to generate fixtures
    3. Camera parameters used in fixture generation
    4. Full VAE, text encoder, and scheduler setup
    """
    
    def test_full_pipeline_match_image_vae_embedding(self):
        """
        Test that VAE encoding produces matching image_vae_embedding.
        
        This would require:
        - Loading AutoencoderKLWan with pre-trained weights
        - The exact input image used in fixture generation
        - Matching VAE settings (scaling factor, etc.)
        """
        # TODO: Implement when pre-trained weights and input data are available
        self.skipTest("Requires pre-trained VAE weights and original input image")
        
    def test_full_pipeline_match_control_condition_embedding(self):
        """
        Test that camera control encoding produces matching control_condition_embedding.
        
        This would require:
        - Camera control encoder implementation (PlÃ¼cker -> 36 channels)
        - The exact camera parameters used in fixture generation
        - Matching encoding settings
        """
        # TODO: Implement when camera encoder is complete
        self.skipTest("Requires camera control encoder implementation")
        
    def test_full_pipeline_match_all_intermediate_features(self):
        """
        End-to-end test matching all fixture outputs.
        
        This would:
        1. Load all pre-trained components (VAE, transformer, text encoder)
        2. Process the same inputs as fixture generation
        3. Validate all intermediate features match:
           - image_vae_embedding
           - control_condition_embedding
           - patchify_tensor
           - patch_sequence
           - All transformer block outputs
        """
        # TODO: Implement when all components and weights are available
        self.skipTest("Requires complete pipeline with pre-trained weights")


if __name__ == "__main__":
    unittest.main()
