# Copyright 2025 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Tests for WanFunControl pipeline and transformer.

This test suite validates that the diffusers implementation of Wan Fun-Control
produces intermediate features that match the reference fixtures generated by
DiffSynth-Studio's WanVideo pipeline.

Fixture path: tests/fixtures/wan21_fun_v11_control_camera.pt
Fixture metadata: tests/fixtures/wan21_fun_v11_control_camera.json

Expected intermediate features:
1. image_vae_embedding: [1, 20, 21, 60, 104] - VAE encoded input image
2. control_condition_embedding: [1, 36, 21, 60, 104] - Camera control embeddings
3. patchify_tensor: [1, 1536, 21, 30, 52] - Patchified output with control
4. patch_sequence: [1, 32760, 1536] - Reshaped patches (32760 = 21*30*52)
5. blocks: Transformer block outputs at intervals (0, 6, 12, 18, 24)
"""

import json
import unittest
from pathlib import Path

import torch

from diffusers.models import WanFunControlTransformer3DModel


class WanFunControlFixtureTests(unittest.TestCase):
    """Test WanFunControl implementation against reference fixtures."""

    @classmethod
    def setUpClass(cls):
        """Load fixtures once for all tests."""
        fixture_dir = Path(__file__).parent.parent.parent.parent.parent / "tests" / "fixtures"
        fixture_path = fixture_dir / "wan21_fun_v11_control_camera.pt"
        metadata_path = fixture_dir / "wan21_fun_v11_control_camera.json"
        
        if not fixture_path.exists():
            raise FileNotFoundError(f"Fixture not found: {fixture_path}")
        
        cls.fixture = torch.load(fixture_path, map_location="cpu")
        
        with open(metadata_path) as f:
            cls.metadata = json.load(f)
        
        print("\n=== Fixture Metadata ===")
        print(f"Model: {cls.metadata['model_id']}")
        print(f"Prompt: {cls.metadata['prompt']}")
        print(f"Video: {cls.metadata['num_frames']} frames, {cls.metadata['height']}x{cls.metadata['width']}")
        print(f"Block interval: {cls.metadata['block_interval']}")
        print("\n=== Expected Shapes ===")
        for name, info in cls.metadata['tensors'].items():
            print(f"{name}: {info['shape']}")
        print(f"\nBlocks: {list(cls.metadata['blocks'].keys())}")
        
    def test_fixture_loaded(self):
        """Verify fixture data is loaded correctly."""
        self.assertIn("image_vae_embedding", self.fixture)
        self.assertIn("control_condition_embedding", self.fixture)
        self.assertIn("patchify_tensor", self.fixture)
        self.assertIn("patch_sequence", self.fixture)
        self.assertIn("blocks", self.fixture)
        
    def test_image_vae_embedding_shape(self):
        """Verify image VAE embedding has expected shape."""
        expected_shape = [1, 20, 21, 60, 104]
        actual_shape = list(self.fixture["image_vae_embedding"].shape)
        self.assertEqual(actual_shape, expected_shape)
        
    def test_control_condition_embedding_shape(self):
        """Verify control condition embedding has expected shape."""
        expected_shape = [1, 36, 21, 60, 104]
        actual_shape = list(self.fixture["control_condition_embedding"].shape)
        self.assertEqual(actual_shape, expected_shape)
        
    def test_patchify_tensor_shape(self):
        """Verify patchify tensor has expected shape."""
        expected_shape = [1, 1536, 21, 30, 52]
        actual_shape = list(self.fixture["patchify_tensor"].shape)
        self.assertEqual(actual_shape, expected_shape)
        
    def test_patch_sequence_shape(self):
        """Verify patch sequence has expected shape."""
        expected_shape = [1, 32760, 1536]
        actual_shape = list(self.fixture["patch_sequence"].shape)
        self.assertEqual(actual_shape, expected_shape)
        # Verify 32760 = 21 * 30 * 52
        self.assertEqual(21 * 30 * 52, 32760)
        
    def test_transformer_model_can_be_created(self):
        """Verify WanFunControlTransformer3DModel can be instantiated."""
        # Model configuration based on Wan2.1-Fun-V1.1-1.3B-Control-Camera
        # This is a smaller variant of the 14B model
        config = {
            "patch_size": (1, 2, 2),
            "num_attention_heads": 12,  # Smaller model, fewer heads
            "attention_head_dim": 128,
            "in_channels": 20,  # Image VAE latent channels
            "control_channels": 36,  # Control condition channels
            "out_channels": 20,
            "text_dim": 4096,
            "freq_dim": 256,
            "ffn_dim": 4608,  # 12 * 128 * 3
            "num_layers": 28,  # Fewer layers for 1.3B model
            "cross_attn_norm": True,
            "qk_norm": "rms_norm_across_heads",
            "eps": 1e-6,
        }
        
        try:
            model = WanFunControlTransformer3DModel(**config)
            self.assertIsNotNone(model)
            
            # Verify the model has the patchify method
            self.assertTrue(hasattr(model, "patchify"))
            
            # Verify the model accepts control_camera_latents in forward
            import inspect
            sig = inspect.signature(model.forward)
            self.assertIn("control_camera_latents", sig.parameters)
            
            print(f"\n✓ WanFunControlTransformer3DModel created successfully")
            print(f"  Total params: ~{sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")
            
        except Exception as e:
            self.fail(f"Failed to create WanFunControlTransformer3DModel: {e}")
        
    def test_patchify_accepts_control_latents(self):
        """Test that patchify method accepts control camera latents."""
        config = {
            "patch_size": (1, 2, 2),
            "num_attention_heads": 12,
            "attention_head_dim": 128,
            "in_channels": 20,
            "control_channels": 36,
            "out_channels": 20,
            "num_layers": 4,  # Small model for testing
        }
        
        model = WanFunControlTransformer3DModel(**config)
        
        # Create dummy inputs matching fixture dimensions (but much smaller for speed)
        batch, frames, height, width = 1, 21, 60, 104
        image_latents = torch.randn(batch, 20, frames, height, width)
        control_latents = torch.randn(batch, 36, frames, height, width)
        
        # Test patchify
        with torch.no_grad():
            output = model.patchify(image_latents, control_latents)
        
        # Expected output shape: [batch, inner_dim, frames, height//2, width//2]
        # inner_dim = num_attention_heads * attention_head_dim = 12 * 128 = 1536
        expected_shape = [1, 1536, 21, 30, 52]
        actual_shape = list(output.shape)
        
        self.assertEqual(actual_shape, expected_shape, 
                        f"Patchify output shape mismatch. Expected {expected_shape}, got {actual_shape}")
        
        print(f"\n✓ Patchify output shape matches fixture: {actual_shape}")


class WanFunControlImplementationTests(unittest.TestCase):
    """Tests for implementing features to match fixture outputs."""
    
    def test_camera_control_encoder_placeholder(self):
        """Placeholder test for camera control encoder implementation."""
        # TODO: Implement camera control encoder that converts:
        # Plücker ray embeddings [B, V, H, W, 6] → [B, 36, V, H, W]
        # 
        # The encoder likely:
        # 1. Reshapes Plücker embeddings from [B, V, H, W, 6] to [B, 6, V, H, W]
        # 2. Applies some Conv3d or MLP layers to expand 6 → 36 channels
        # 3. The 36 channels might be 6 groups of 6 features each, or some other encoding
        self.skipTest("Camera control encoder not yet implemented")
        
    def test_vae_encoding_placeholder(self):
        """Placeholder test for VAE encoding of input image."""
        # TODO: Implement VAE encoding that produces:
        # Input image → VAE encoder → [1, 20, 21, 60, 104]
        # 
        # This requires:
        # 1. Loading AutoencoderKLWan
        # 2. Encoding input image/video frames
        # 3. Verifying output matches fixture
        self.skipTest("VAE encoding validation not yet implemented")
        
    def test_full_forward_pass_placeholder(self):
        """Placeholder test for full forward pass through transformer."""
        # TODO: Implement full forward pass that validates:
        # 1. Text encoder produces correct embeddings
        # 2. VAE encodes image correctly
        # 3. Camera encoder produces control latents
        # 4. Patchify combines them correctly
        # 5. Transformer blocks produce matching intermediate outputs
        self.skipTest("Full forward pass not yet implemented")


if __name__ == "__main__":
    unittest.main()
